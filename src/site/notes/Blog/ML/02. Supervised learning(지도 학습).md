---
{"dg-publish":true,"permalink":"/blog/ml/02-supervised-learning/"}
---

지도학습은 입력값(input data) x에 대한 출력값(label) y를 맞히도록 하는 함수 $f: x → y$ 를 학습하는 것

대표적인 예시로 linear regression이 있다.

# Linear Regression

예시를 먼저 들어보면,
먼저 아래와 같은 집 크기에 대한 집 값이 있다고 가정해보자.

| 집 크기(㎡) | 집 값(천만원) |
| ------- | -------- |
| 30      | 2,000    |
| 50      | 3,500    |
| 80      | 5,500    |
| 100     | 7,600    |

우리는 이때 60평에 해당하는 집의 값을 알고 싶다.

그렇기 위해서는 두 관계를 예측할 수 있는 식을 만들어야 한다.
$$f: x → y$$
이 선형 회귀식은 x와 y의 관계를 나타내주는 식으로 n차 방정식으로 구성할 수 있다.

회귀식의 일반 형태는 다음과 같다.

$y = wx + by$

- x: 입력 데이터(독립 변수)
- y: 출력 데이터(종속 변수)
- w: 기울기(가중치)
- b: 절편(바이어스)

하지만 이런 일차 회귀식은 x와 y간의 관계를 충분히 포함하지 못할 수 있다.
집의 크기에 대한 집 값이 100평 이후에서 폭팔적으로 높아질 수 있기 때문이다. 

이럴 때 입력 데이터 x의 차수를 늘려가면서 y를 추론할 수 있지만 이후 설명할 과적합 현상이 발생할 수 있기 때문에 신중히 조절해야 한다.

또한 입력 데이터가 하나 말고 여러개가 들어올 수 있다.

$x_1$: 집 크기
$x_2$: 주변 범죄율

이전에 말한 입력 데이터가 하나인 경우에는 2차원 평면에서 $f$가 직선이나 곡선의 형태로 존재할 수 있다.
하지만 입력 데이터의 종류가 2개라면 $f$가 평면이나 곡면의 형태로 출력될 수 있다.

$$ y = ax_1 + bx_2 + c$$
$x \in \mathbb{R}^2$ : Features/Input
$y \in \mathbb{R}$ : Label/Class/Target/Output

즉, 지도(Supervised)를 통해 정답 y를 기반으로 input: x → expected output: $\hat{y}$ 를 맵핑하는 함수를 학습한다.

물론 더 많은 입력값을 가질 수 있다.
$x \in \mathbb{R}^d$ for large d

$$x = \begin{bmatrix}
x_1 \\
x_2 \\
x_3 \\
\vdots \\
x_d
\end{bmatrix}
\begin{array}{l}
\text{--- living size} \\
\text{--- lot size} \\
\text{--- \# floors} \\
\text{--- condition} \\
\text{--- zip code} \\
\vdots
\end{array}
$$

$$ h(x) = \theta_1 x_1 + \theta_2 x_2 + ... + \theta_d x_d + \theta_0 $$
여기서 $\theta$는 각각의 입력에 대해 얼마나 가중치를 줄 지 정하는 파라미터(매개변수)이다.

이 파라미터를 찾아내는 것이 곧 가설함수 h를 배운다는 것이다.

---

$$ \begin{array}{|c|c|c|c|c|}
\hline
 & \text{size} & \text{bedrooms} & \text{lot size} & \text{Price} \\
\hline
x^{(1)} & 2104 & 4 & 45k & y^{(1)}=400 \\
x^{(2)} & 2500 & 3 & 30k & y^{(2)}=900 \\
\hline
\end{array}
$$

위와 같은 표를 예시로 들어보자.
그럼 이제 input data는 3개의 차원(특징)을 가진다.

이를 가설함수 $h$로 나타내면

$$h(x) = \theta_0 x_0 + \theta_1 x_1 + \theta_2 x_2 + \theta_3 x_3$$
이렇게 나타낼 수 있다. 이때 $x_0$은 1이라 표현하여 아래처럼 나타낼 수 있다.

$$ h(x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \theta_3 x_3 $$
따라서 이렇게 표현 가능하다.
$$ h(x) = \sum_{j=0}^{3} \theta_j x_j $$
열 백터로 표현해보면 아래와 같다.
$$ \theta = \begin{pmatrix}
\theta_0 \\
\theta_1 \\
\theta_2 \\
\theta_3
\end{pmatrix}
\quad \text{and} \quad
x^{(1)} = \begin{pmatrix}
x_0^{(1)} \\
x_1^{(1)} \\
x_2^{(1)} \\
x_3^{(1)}
\end{pmatrix}
= \begin{pmatrix}
1 \\
2104 \\
4 \\
45
\end{pmatrix}
\quad \text{and} \quad y^{(1)} = 400
$$
---
다시 Linear Regression을 문제 정의 해보면

$\theta \in \mathbb{R}^{d+1}$로 `파라미터화` 되어 있는 함수 $h: x \in \mathbb{R}^d → y \in \mathbb{R}$를 N개의 데이터 쌍으로부터 학습
$$ h(x) = \sum_{j=0}^{d} \theta_j x_j \approx y$$

결국 어떻게 파라미터를 찾는 지가 결국 문제를 푸는 방법

# Least Square Methods

Goal: h(x)를 y랑 가깝게 되도록 $\theta_j$를 배운다.

최소제곱법은 목적함수 objective function J를 아래와 같이 정의하고
$$ J(\theta) = \frac{1}{2} \sum_{i=1}^{N}(h_\theta(x^{(i)}) - y^{(i)})^2 $$
i번째 데이터에 대해 $\hat{y}^{(i)}와 y^{(i)}$의 차이를 구한다.

왜 이런 형태인가?

1. 그냥 빼버리게 되면 부호(+, -)에 따른 처리가 어려움
2. $|\hat{y}^{(i)} - y^{(i)}|$ 는 미분 불가능한 형태로 이후 나올 최적화 과정에 사용할 수 없다.
3. 그래서 여기에 제곱을 붙여 미분 가능 형태로 만든 것이다.


이 목적함수 J를 최소로 만드는 파라미터를 찾으면 된다.

> [!info] 왜 목적함수로 부르는가?
> 항상 최소화의 대상이 아닐 수 있다. 
> 회귀에서는 오차의 최소화
> 분류에서는 해당 확률이 되도록 최대화

그럼 어떻게 최소제곱법에서 $J(\theta)$를 최소화하는 $\theta$를 찾을 것인가?

-  convex 함수일 경우, 미분하여 0이되는 해를 찾기
	- 이는 수식을 통한 문제 해결 방식
	- 계산 비용이 많을 수 있음
- 경사 하강법 Gradient descent
	- 미분하여 0이 되는 지점을 찾음
	- 기울기의 반대 방향으로 조금씩 이동

이 경사 하강법에 대해 더 자세히 설명해보면,
$J'(\theta) = 0$을 바로 찾는 것이 아니라 초기값 $\theta_0$에서 시작해서 미분 방향으로 조금씩 이동하는 방법이다.

$$ \theta_{new} = \theta_{old} - \alpha \frac{dJ(\theta)}{d\theta} $$

미분 반대 방향으로 조금씩 이동해 minimal point로 이동한다.

이때 이 point가 global minimum이 아니라 local minimum일 수 있다.
따라서 초기값을 여러 군데로 두어 찾아야 한다.

만약 변수가 둘 이상인 함수에서 최소값을 찾으려면 편미분을 이용한다.

Next
