---
{"dg-publish":true,"permalink":"/blog/ml/02-02-classification/"}
---

분류의 출력값(output)은 Category 값에 해당한다.
즉 출력값의 형태가 이산변수에 해당하도록 만들어줘야 한다.

# 선형 분류기

$$ f(x) = w^T x = \begin{bmatrix}
W_0 \\
W_1 \\
W_2
\end{bmatrix}^T 

\begin{bmatrix}
1 \\
x_1 \\
x_2 
\end{bmatrix}
$$ 벡터로 표현한 선형 분류기이다.

이제 만약 $f(x) \ge 0$ 이면 $C_1$, 아니면 $C_2$로 분류 가능하다.

# Logistic Regression

위 선형 분류 방식을 좀 더 부드럽게 만들어주는 방식이 Logistic 함수를 사용한 Logistic Regression이다.

> [!info] 주의
> 이름은 회귀지만 실제로는 분류를 사용하는 것이다.

$$\Large \sigma(x) = \frac{1}{1+e^{-x}}$$
어떤 Class에 속하는 지에 대한 확률값

![Pasted image 20250713194816.png](/img/user/Pasted%20image%2020250713194816.png)

이렇게 출력되는 로지스틱 함수 $\sigma$에 input으로 선형회귀 함수 $W^T x$를 넣어주면
$$ f(x) = \sigma(W^T x) $$
![[Pasted image 20250713195700.png \|Pasted image 20250713195700.png ]]

$$f(x) = \sigma(x_1 + x_2 - 2)$$
$$f(x=[2,2]) = \frac{1}{1+e^{2+2-2}} = 0.12$$
로지스틱 회귀에서는 단순하게 SSE나 RMSE를 사용해 학습하지 않는다.
- SSE는 사용할 수 있지만 오차를 과장하고 RMSE보다는 밑에 설명할 MLE가 더 애매한 경계를 잘 찾는다.

# MLE(Maximum likelihood Estimation)

MLE는 기존 LSM(Least Squared Method)들과 다르게 확률값을 키우도록 학습한다.

+ 회귀는 연속적인 값을 사용하므로 직접 오차를 계산하지만
+ 분류는 카테고리 값으로 나타내야 하므로 `확률`로 바꿔 학습한다.

그럼 어떤 확률값을 키우도록 학습하는 것인가?

$$ \Large P(θ|D) = \frac{P(D|θ) \cdot P(θ)}{P(D)} $$
H: Hypothesis(가설)
D: Evidence(데이터)

P(θ | D): Posterior(사후확률)
- 보통 이 Posterior를 구하는 것이 Best이긴 하지만 Posterior는 P(θ)와 P(D)를 가지고 있다. 
	- 뒤에 설명할 P(θ)는 구할 수 없다.

P(D | θ): Likelihood(우도)
- 그래서 이 likelihood 값을 증가시켜 Posterior를 증가시킨다.
- 이는 θ라는 파라미터를 가질 때 이 data가 D처럼 분포될 확률을 나타낸다.

P(D): Evidence
- 상수

P(θ): Prior(사전확률)
- 해당 파라미터를 가질 수 있는 모델의 개수가 무한에 가까운데 그 중 이 θ에 대한 확률을 나타내는 것은 불가능하다.

---
따라서 로지스틱 회귀에서는 MLE로 학습하여 모든 데이터에 대해 확률 값을 올려야 한다.
즉, 우리가 가지고 있는 파라미터 θ를 가지고 현재 내가 데이터의 정답을 맞출 확률(likelihood)을 최대화하는 것이 목표이다.

i번째 데이터에 대해서만 생각해보면, Class 1일 확률이 높아지도록 아래의 Likelihood를 최대화하면 된다.
$$ p(y{(i)} = C_1 | \theta) $$

$p(y=1|x;w) = h_w(x)$ 
- x가 w라는 파라미터로 구성되었을 때 y=1인 class로 분류될 확률

$p(y=0|x;w) = 1 - h_w(x)$
- x가 w라는 파리미터로 구성되었을 때 y=0인 class로 분류될 확률
- 이진 분류이므로 위에서 구한 $h_w(x)$에 1을 빼면 된다.

각 독립적인 데이터의 모음에 대한 확률값은 아래와 같이 표현할 수 있다.

$$L(w) = P(y|x;w) = \prod_{i=1}^{n} P\left(y^{(i)}|x^{(i)};w\right)$$
i번째 입력 데이터 x가 w라는 파라미터를 가졌을 때 y를 나타낼 확률을 모두 곱한 것이다.
이때 $h_w(x^{(i)})^{y^{(i)}}$ 를 $P(y^{(i)} = 1 | x^{(i)} ; w)$ 대신 사용할 수 있다. 정답 label $y^{(i)}$은 0 또는 1 이므로 아래와 같이 두 확률의 곱으로 표현 가능하다. 
$$ = \prod_{i=1}^{n} h_w\left(x^{(i)}\right)^{y^{(i)}} \left(1 - h_w\left(x^{(i)}\right)\right)^{1 - y^{(i)}}$$ 그리고 이 식에 로그를 취하여 아래와 같이 나타내면 곱셈 기호가 덧셈 기호로 변한다.

$$ l(w) = -\log L(w) = -\log \prod_{i=1}^{n} h_w\left(x^{(i)}\right)^{y^{(i)}} \left(1 - h_w\left(x^{(i)}\right)\right)^{1 - y^{(i)}} $$
이때 log 앞의 - 부호를 붙였는데 이는 로그 안쪽 값이 확률값이므로 
log 출력 결과 -가 붙어 나와 -  x  - = +로 Log 함수의 단조 증가성을 유지해준다. 

$$ = -\sum_{i=1}^{n} y^{(i)} \log h_w\left(x^{(i)}\right) + \left(1 - y^{(i)}\right) \log\left(1 - h_w\left(x^{(i)}\right)\right) $$

