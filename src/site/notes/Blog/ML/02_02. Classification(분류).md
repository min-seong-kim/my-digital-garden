---
{"dg-publish":true,"permalink":"/blog/ml/02-02-classification/"}
---

분류의 출력값(output)은 Category 값에 해당한다.
즉 출력값의 형태가 이산변수에 해당하도록 만들어줘야 한다.

# 선형 분류기

$$ f(x) = w^T x = \begin{bmatrix}
W_0 \\
W_1 \\
W_2
\end{bmatrix}^T 

\begin{bmatrix}
1 \\
x_1 \\
x_2 
\end{bmatrix}
$$ 벡터로 표현한 선형 분류기이다.

이제 만약 $f(x) \ge 0$ 이면 $C_1$, 아니면 $C_2$로 분류 가능하다.

# Logistic Regression

위 선형 분류 방식을 좀 더 부드럽게 만들어주는 방식이 Logistic 함수를 사용한 Logistic Regression이다.

> [!info] 주의
> 이름은 회귀지만 실제로는 분류를 사용하는 것이다.

$$\Large \sigma(x) = \frac{1}{1+e^{-x}}$$
어떤 Class에 속하는 지에 대한 확률값

![Pasted image 20250713194816.png](/img/user/Pasted%20image%2020250713194816.png)

이렇게 출력되는 로지스틱 함수 $\sigma$에 input으로 선형회귀 함수 $W^T x$를 넣어주면
$$ f(x) = \sigma(W^T x) $$
![[Pasted image 20250713195700.png \|Pasted image 20250713195700.png ]]

$$f(x) = \sigma(x_1 + x_2 - 2)$$
$$f(x=[2,2]) = \frac{1}{1+e^{2+2-2}} = 0.12$$
로지스틱 회귀에서는 단순하게 SSE나 RMSE를 사용해 학습하지 않는다.
- SSE는 사용할 수 있지만 오차를 과장하고 RMSE보다는 밑에 설명할 MLE가 더 애매한 경계를 잘 찾는다.

# MLE(Maximum likelihood Estimation)

MLE는 기존 LSM(Least Squared Method)들과 다르게 확률값을 키우도록 학습한다.

+ 회귀는 연속적인 값을 사용하므로 직접 오차를 계산하지만
+ 분류는 카테고리 값으로 나타내야 하므로 `확률`로 바꿔 학습한다.

그럼 어떤 확률값을 키우도록 학습하는 것인가?

$$ \Large P(θ|D) = \frac{P(D|θ) \cdot P(θ)}{P(D)} $$
H: Hypothesis(가설)
D: Evidence(데이터)

P(θ | D): Posterior(사후확률)
- 보통 이 Posterior를 구하는 것이 Best이긴 하지만 Posterior는 P(θ)와 P(D)를 가지고 있다. 
	- 뒤에 설명할 P(θ)는 구할 수 없다.

P(D | θ): Likelihood(우도)
- 그래서 이 likelihood 값을 증가시켜 Posterior를 증가시킨다.
- 이는 θ라는 파라미터를 가질 때 이 data가 D처럼 분포될 확률을 나타낸다.

P(D): Evidence
- 상수

P(θ): Prior(사전확률)
- 해당 파라미터를 가질 수 있는 모델의 개수가 무한에 가까운데 그 중 이 θ에 대한 확률을 나타내는 것은 불가능하다.

---
따라서 로지스틱 회귀에서는 MLE로 학습하여 모든 데이터에 대해 확률 값을 올려야 한다.
즉, 우리가 가지고 있는 파라미터 θ를 가지고 현재 내가 데이터의 정답을 맞출 확률(likelihood)을 최대화하는 것이 목표이다.

